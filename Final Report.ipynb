{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO LIST:\n",
    "\n",
    "1. **Introduction** where you discuss the business problem and who would be interested in this project. **X**\n",
    "2. **Data** where you describe the data that will be used to solve the problem and the source of the data. **X**\n",
    "3. **Methodology** section which represents the main component of the report where you discuss and describe any exploratory data analysis that you did, any inferential statistical testing that you performed, if any, and what machine learnings were used and why.\n",
    "4. **Results** section where you discuss the results.\n",
    "5. **Discussion** section where you discuss any observations you noted and any recommendations you can make based on the results.\n",
    "6. **Conclusion** section where you conclude the report.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Capstone Project: Bar Crawler\n",
    "\n",
    "*Note: For the most detail and full, actual code used to pull, clean, analyze, and map data, see the 'Capstone Project' notebook. The purpose of this report is to serve as a higher-level, more easily-communicable description of the problem; data; methodology; results; discussion; and review/conclusions.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction/Business Problem\n",
    "\n",
    "Consider one of these scenarios:\n",
    "* You are the owner of a travel blog, or\n",
    "* You work for a travel company writing articles, or\n",
    "* You work for a company doing research into travel and/or the New York City night life, or\n",
    "* (A different alternative:) You're a prospective bar or night club owner looking to profit off being in an active nightlife area.\n",
    "\n",
    "Maybe you want to write an article or report on what life was like before COVID-19, or you're prepping some content for all the people planning to travel once it's over. In any case, our goal is to write an \"N Places...\" article, because who doesn't love a good 12 Places to Take a Really Cool Selfie in NYC article?\n",
    "\n",
    "What type of list would be best? Well, why not one that facilitates an adventurous social gathering that invovles a core group and the potential to meet lots of people? The answer is clear: Bar Crawl Spots in NYC! Who doesn't love a good bar crawl?\n",
    "\n",
    "To do this, we want to give some travelers good advice when visiting New York City, but we're not locals, or very well-versed in the night-life. In fact, this writer is from and residing in California. So we want to use data to help us.\n",
    "\n",
    "**Our Problem: What _bar crawl spots_ are there in New York City that we can recommend to prospective travelers?**\n",
    "\n",
    "The answer to this will help us write a great article or report with actual maps of the areas and actual places to go to. Our number of clusters *k* will be the \"N\" bar crawl areas, but the actual value of k will depend on the *quality* of the clusters: do they contain actual bars, and are they close together?; as well as the *quantity*: are regions that could be considered one crawl split up into two, or three? Are several different crawls being lumped into one?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "\n",
    "The data necessary for this project is actually quite minimal. We only need three different sources of data, two of which are publicly available through free-to-use APIs.\n",
    "\n",
    "### Source #1: Foursquare (Venues and their Locations)\n",
    "\n",
    "In order to discover bar crawl locations, we needed a substantial amount of actual venues (goal was set to be at least 500). In order to get this data, Foursquare's Places API was utilized. The constraints on the venue data used were fairly minimal: \n",
    "\n",
    "1. Venues must be tagged as a 'Nightlife' venue according to Foursquare's documentation (see: https://developer.foursquare.com/docs/build-with-foursquare/categories/);\n",
    "\n",
    "2. Each venue must be unique (based on name or address);\n",
    "\n",
    "3. Locations must be within New York City (ignore adjacent venues even if they are nearby others within city limits to establish a definite boundary); and\n",
    "\n",
    "4. All venues must have valid latitude/longitude coordinates.\n",
    "\n",
    "For a more detailed description of the methods used to pull and clean the data, see **Methodology**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source #2: NYU Spacial Data Repository (Neighborhoods in NYC and their Locations)\n",
    "\n",
    "Due to a limitation of the Foursquare Places API which will be discussed in further detail in the Methodology section, it became necessary to have many different latitude/longitude coordinates within NYC. This was made possible by a freely available dataset actually used in the \"Segmenting and Clustering Neighborhoods in New York City\" lab, which comes from the NYU Spacial Data Repository. The data actually used in the lab is from IBM themselves, who very courteously (for the purposes of the lab work) cleaned the data.\n",
    "\n",
    "Link to NYU Spacial Data Repository page: https://geo.nyu.edu/catalog/nyu_2451_34572\n",
    "\n",
    "Link to this course's raw, cleaned dataset: https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/labs/newyork_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source #3: Folium (Maps of Bar Crawl Spots)\n",
    "\n",
    "Folium was used to generate maps of all discovered venues; all venues, clustered; and individual clusters of venues. Folium is incredibly easy to use, and has all the features we need for this analysis. Much of the code used to plot the venues onto Folium maps was taken from the \"Segmenting and Clustering Neighborhoods in New York City\" lab, and modified to fit the needs of this analysis.\n",
    "\n",
    "Link to Folium's documentation: https://python-visualization.github.io/folium/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods used in this project can be described in four approximate steps:\n",
    "\n",
    "1. Data **Pulling**\n",
    "2. Data **Cleaning**\n",
    "3. Data **Analysis**\n",
    "4. Data **Visualization**\n",
    "\n",
    "The following four subsections will correspond to each of these approximate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Step 1: Data Pulling\n",
    "\n",
    "As discussed previously, the Foursquare Places API was used to discover Nightlife venues in New York City. The API call required the following parameters:\n",
    "\n",
    "* Authentication parameters: Client_Id, and Client_Secret\n",
    "* Version: just a date; 01/21/2021 was used\n",
    "* Location: either coordinates or name of place; the former were used (see below)\n",
    "* Query: search query; to be as general as possible, 'bar' was used\n",
    "* Limit: maximum value is 50, which was used\n",
    "* CategoryId: not a necessary parameter, but key for our analysis to restrict results to 'Nightlife' venues\n",
    "\n",
    "An obstacle to pulling all the data needed for the analysis was the limit on results from a single search query to 50. Running a query on 'New York City' as the location would only give 50 results nearby the coordinates of the city, which was around the Manhattan area. In order to overcome this limitation, the NYU Spatial Data Repository dataset was used. In it, all 306 neighborhoods across the 5 boroughs are detailed with each of their respective coordinates.\n",
    "\n",
    "Queries to the Places API were done for each of the 306 coordinates in the Neighborhoods dataset using the requests library. The results were transformed into json format; turned into a pandas DataFrame; and stored in a list of the resulting DataFrames. The total number of results was just under the expected 306 * 50 = 15,300.\n",
    "\n",
    "Finally, the list of pandas DataFrames were concatenated into one complete DataFrame of all search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Step 2: Data Cleaning\n",
    "\n",
    "The complete DataFrame contained 16 features, and (as stated) just under 15,300 rows. To meet the set constraints on the venues data (of the Nightlife category; within NYC; unqiue; and with valid coordinates), cleaning was necessary. The following were performed to achieve these contraints:\n",
    "\n",
    "1. As previously stated, CategoryId was specified in the search queries to ensure each venue was of the 'Nightlife' category.\n",
    "\n",
    "2. The pandas \"drop_duplicates\" method was used on the DataFrame of all results, specifically on the names and addresses of all venues. This ensured venues with either the same name or address were dropped. Only the first of the duplicates was kept. Notably, this eliminated chains (different venues sharing the same name at different addresses); an unfortunate consequence, but since so many results were likely to overlap with only small changes in coordinates across NYC, the decision was made to focus on unique (and perhaps more 'local') destinations instead of 'completeness' of data.\n",
    "\n",
    "3. The NYC Neighborhoods dataset was used to ensure the venues were within NYC. Due to the 'city' feature of the results actually having values spanning borough, neighborhood, or city, the Neighborhoods dataset was turned into a list of the names of all the unique neighborhoods and boroughs; 'New York' and 'New York City' were also added to keep those venues which were properly labeled as the city. Pandas' selection methods and the \"isin\" method were used to accomplish this.\n",
    "\n",
    "4. To ensure all venues had valid coordinates, all rows of the resulting DataFrame which had \"NA\" values for either latitude or longitude were dropped from the DataFrame by using pandas' \"dropna\" method.\n",
    "\n",
    "Finally, the cleaned DataFrame was saved locally using pickle. Additionally, because some features were not necessary, a 'slimmed down' version with these columns removed was also saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Step 3: Data Analysis\n",
    "\n",
    "The k-Means machine learning algorithm was used to discover bar crawl destinations. While potentially not the best possible choice, it was chosen for being easy to use, flexible, and especially simple to explain (number of clusters = number of bar crawl destinations). The Scikit-Learn library was used to apply the algorithm to the data.\n",
    "\n",
    "The process was remarkably simple. First, a separate DataFrame containing only the coodinates of each venue was made which aligned with the original DataFrame based on its index. Next, this data along with a chosen value of k (see below) were passed to the KMeans function from the Scikit-Learn library. This produced an array of Cluster Labels corresponding to the unique indices of the rows. Finally, this array of labels was inserted into a copy of the DataFrame containing the venues in a new 'Cluster Label' category. Since the indices matched, a simple insert was all that was needed to create the final DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Picking k, and Error Analysis\n",
    "\n",
    "A quirk of this project is the lack of error analysis. A common way to discover the 'correct' value of k is to plot accuracy of the model over values of k within a range, and find the 'elbow point' - the value of k at which increases in k no longer yield gains in accuracy. However, this project is unique in that *there is no 'correct' value of k.* The definition of a \"bar crawl destination\" is not technical or well-defined, at least quantitatively. Therefore, the process of choosing the final value of k was a heuristic, not a quantitative approach. The two criteria used were: (1) k being a \"nice, round\" number, and (2) the resulting clusters being at least hypothetically walkable.\n",
    "\n",
    "Therefore, there is no dedicated error analysis section to this report, and no elbow points discovered. The final value of k was decided based on some trial-and-error and gut feeling. Ultimately, the chosen value of k was 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Step 4: Data Visualization\n",
    "\n",
    "As Folium is built to be easy to use and quite lightweight, all that was needed to construct a map of all venues in the dataset in NYC was to assign a point of a uniform color to each coordinate. In order to construct a map of all venues separated into their respective clusters, different colors were generated from Matplotlib's 'cm' and 'colors' modules, and assigned to each cluster.\n",
    "\n",
    "However, both of these maps were not very easy to read, and therefore not very useful to anyone looking to use this analysis for any of the purposes described at the beginning of this report. Therefore, additional code was written to instead plot a single selected cluster at a time to the map of NYC to analyze one bar crawl destination at a time, and finally generate recommendations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
